{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements.\n",
    "%pip install \"build>=1.0\" \"torch>=2.0\" \"transformers\" \"../../libs/buildlib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import buildlib as buildlib\n",
    "from transformers.models.bert.modeling_bert import BertModel\n",
    "from transformers.models.bert.tokenization_bert_fast import BertTokenizerFast\n",
    "\n",
    "from build import ProjectBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In order to create a custom text embedding service, we need to create a Docker image and deploy it. In order to build this custom image, we put custom data (a `data/` directory) and logic (an `embed.py`, plus a `requirements.txt` specifying any dependencies) into the `build/` directory, and then we use the `buildlib` to run the build process (`buildlib.build()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear out `build/`\n",
    "\n",
    "Let's start with a clean slate by deleting and recreating an empty `build/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUILD_DIR = Path(\".\").resolve().parents[1] / \"build\"\n",
    "shutil.rmtree(BUILD_DIR)\n",
    "BUILD_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "For this example, we will use the [`e5-base-v2`](https://huggingface.co/intfloat/e5-base-v2) model from Microsoft. We'll demonstrate how to preload the model weights directly into our Docker image for simpler deployment.\n",
    "\n",
    "Rather than just using the [`transformers`](https://pypi.org/project/transformers/) library directly, we'll also demonstrate how to include a custom library into the Docker image by building a [pure Python wheel](https://packaging.python.org/en/latest/guides/distributing-packages-using-setuptools/#pure-python-wheels) and including that in the `data/` alongside the model weights. The library we demonstrate on is `embed_lib/`, a small example library we wrote to call the E5 model via `transformers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put model weights into `build/data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the model weights.\n",
    "MODEL_NAME = \"intfloat/e5-base-v2\"\n",
    "DATA_DIR = BUILD_DIR / \"data\"\n",
    "TOKENISER_DIR = DATA_DIR / \"tokenizer\"\n",
    "MODEL_DIR = DATA_DIR / \"model\"\n",
    "\n",
    "print(f\"Downloading {MODEL_NAME} and saving tokenizer and weights to {DATA_DIR}\")\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "model = BertModel.from_pretrained(MODEL_NAME)\n",
    "assert isinstance(model, BertModel)  # This is for typechecking.\n",
    "tokenizer.save_pretrained(TOKENISER_DIR)\n",
    "model.save_pretrained(MODEL_DIR)\n",
    "\n",
    "# Validate that our saved files work by loading from them.\n",
    "tokenizer = BertTokenizerFast.from_pretrained(TOKENISER_DIR)\n",
    "model = BertModel.from_pretrained(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put packaged code into `build/data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package our Python package into a wheel in the `data/` directory.\n",
    "wheel_filename = ProjectBuilder(Path(\"\") / \"embed_lib\").build(\n",
    "    distribution=\"wheel\", output_directory=DATA_DIR\n",
    ")\n",
    "print(f\"Built {wheel_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the logic\n",
    "\n",
    "In order to actually use our custom package, load our model weights, and perform text embedding, we need to implement the core logic of text embedding. This is done by implementing `get_embed_fn()` inside a file called `embed.py`. The function `get_embed_fn` should load model weights and return a function that maps a single input consisting of a `Sequence[str]` into a single 2d `numpy` array of datatype `np.float32`. Below we give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement `get_embed_fn` inside `build/embed.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../../build/embed.py\n",
    "import logging\n",
    "from typing import Callable\n",
    "from typing import cast\n",
    "from typing import Sequence\n",
    "\n",
    "import embed_lib.e5\n",
    "import numpy as np\n",
    "from transformers.models.bert.modeling_bert import BertModel\n",
    "from transformers.models.bert.tokenization_bert_fast import BertTokenizerFast\n",
    "\n",
    "MAX_BATCH_SIZE = 4\n",
    "\n",
    "\n",
    "def get_embed_fn(logger: logging.Logger) -> Callable[[Sequence[str]], np.ndarray]:\n",
    "    # Load the model into memory.\n",
    "    logger.info(\"[get_embed_fn]Loading model from disk to memory\")\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\n",
    "        \"/root/data/tokenizer\", local_files_only=True\n",
    "    )\n",
    "    model = cast(\n",
    "        BertModel, BertModel.from_pretrained(\"/root/data/model\", local_files_only=True)\n",
    "    )\n",
    "    e5_model = embed_lib.e5.E5Model(tokenizer, model)\n",
    "\n",
    "    def _embed(texts: Sequence[str]) -> np.ndarray:\n",
    "        result_tensor = embed_lib.e5.embed(\n",
    "            e5_model=e5_model,\n",
    "            texts=texts,\n",
    "            batch_size=MAX_BATCH_SIZE,\n",
    "            normalize=True,\n",
    "            progress_bar=False,\n",
    "        )\n",
    "        result_array = result_tensor.numpy().astype(np.float32)\n",
    "        return result_array\n",
    "\n",
    "    return _embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a `build/requirements.txt`\n",
    "\n",
    "We also need to specify the requirements for our embedding logic. During the build, we will populate the `BUILD_ROOT` environment variable, which enables you to include custom packages (like your `embed_lib` wheel) in your `requirements.txt` by absolute filepath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../../build/requirements.txt\n",
    "\n",
    "${BUILD_ROOT}/data/embed_lib-1.0.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build!\n",
    "\n",
    "Now that all the pieces are in place inside `build/`, we can trigger a build via `buildlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have all the pieces we need to build our service!\n",
    "list(BUILD_DIR.iterdir()) + list(DATA_DIR.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUILD_DIR = Path(\".\").resolve().parents[1] / \"build\"\n",
    "buildlib.build(build_dir=BUILD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed-standalone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
